{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR B 17L-4108 Assign 3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWECP6io8YdL",
        "outputId": "cafe8a6c-f738-4a64-db25-78cd162a3bcd"
      },
      "source": [
        "import nltk\n",
        "import os\n",
        "import re\n",
        "from collections import OrderedDict\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQT12gC8gpX"
      },
      "source": [
        "def read_folder(folder_path, base_id):\n",
        "    \n",
        "    snow_stemmer = SnowballStemmer(language='english')\n",
        "    stop_list = stopwords.words(\"english\")\n",
        "    tokens = OrderedDict()\n",
        "    nltk.download('words')\n",
        "\n",
        "    i = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            pos = 1\n",
        "            soup = BeautifulSoup(f.read(), \"html.parser\")\n",
        "            text = soup.body.get_text()\n",
        "            if text is not None:\n",
        "                text_toks = nltk.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n",
        "                file_dict = {}\n",
        "                for token in text_toks:\n",
        "                    stem = snow_stemmer.stem(token).lower()\n",
        "                    if stem not in stop_list:\n",
        "                        if stem not in file_dict.keys():\n",
        "                            file_dict[stem] = []\n",
        "                        file_dict[stem].append(pos)\n",
        "                    pos += 1\n",
        "                for key, value in file_dict.items():\n",
        "                    if key not in tokens.keys():\n",
        "                        tokens[key] = {}\n",
        "                    tokens[key][base_id] = value\n",
        "                base_id += 1\n",
        "            i += 1\n",
        "            if i > 5:\n",
        "                break\n",
        "\n",
        "    return OrderedDict(sorted(tokens.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkQG4zGG8uHI"
      },
      "source": [
        "def save_inverted_index(index, id):\n",
        "\n",
        "    f_terms = open('index_' + str(id) + '_terms.txt', 'w')\n",
        "    f_posting = open('index_' + str(id) + '_postings.txt', 'w')\n",
        "    \n",
        "    offset = 0\n",
        "    for key, value in index.items():\n",
        "        out = str(len(value))\n",
        "        prev_file = 0\n",
        "        for k, v in value.items():\n",
        "            out += ' ' + str(k-prev_file)\n",
        "            out += ' ' + str(len(v))\n",
        "            prev_pos = 0\n",
        "            for pos in v:\n",
        "                out += ' ' + str(pos-prev_pos)\n",
        "                prev_pos = pos                \n",
        "            prev_file = k\n",
        "        out += '\\n'\n",
        "\n",
        "        try:\n",
        "            f_terms.write(key + ' ' + str(offset) + '\\n')\n",
        "            f_posting.write(out)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        offset += len(out) + 1\n",
        "\n",
        "    f_terms.close()\n",
        "    f_posting.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIQqL3gy8wT6"
      },
      "source": [
        "def display_dict(dict):\n",
        "    for r, y in dict.items():\n",
        "        print(r, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuWxQObFFxHw"
      },
      "source": [
        "def search(term, id):\n",
        "  stem = SnowballStemmer(language='english').stem(term).lower()\n",
        "\n",
        "  print(stem)\n",
        "\n",
        "  offset = -1\n",
        "  with open('index_' + str(id) + '_terms.txt') as f_terms:\n",
        "      for line in f_terms:\n",
        "          k, off = line.split(' ')\n",
        "          if term == k:\n",
        "              offset = int(off)\n",
        "              break\n",
        "\n",
        "  if offset == -1:\n",
        "      print(\"Not Found\")\n",
        "      return\n",
        "\n",
        "  posting = ''\n",
        "  with open('index_' + str(id) + '_postings.txt') as f_posting:\n",
        "      f_posting.seek(offset)\n",
        "      posting = f_posting.readline()\n",
        "\n",
        "\n",
        "  postings = posting.split(' ')\n",
        "  postings = list(map(int, postings))\n",
        "\n",
        "\n",
        "  prev_file = 0\n",
        "  x = 1\n",
        "  for i in range(postings[0]):\n",
        "      file_id = prev_file + postings[x]\n",
        "      x += 1\n",
        "      print(file_id, ':', end='\\t')\n",
        "      prev_off = 0\n",
        "      for j in range(postings[x]):\n",
        "          x += 1\n",
        "          offset = prev_off + postings[x]\n",
        "          print(offset, end=' ')\n",
        "          prev_off = offset\n",
        "\n",
        "      x += 1\n",
        "      print()\n",
        "      prev_file = file_id\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGy6qfrcA3pM",
        "outputId": "1a94e635-16e9-4df1-ec0f-be3f1ffd9732"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "base_ids = [1, 1162, 2207]\n",
        "choice = int(input(\"Enter 1, 2, 3 to select folder:\"))\n",
        "\n",
        "dic = read_folder(\"corpus1/\" + str(choice), base_ids[choice-1])\n",
        "# display_dict(dic)\n",
        "save_inverted_index(dic, choice)\n",
        "search('fine', choice)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Enter 1, 2, 3 to select folder:1\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "fine\n",
            "2 :\t2 4 \n",
            "4 :\t2 4 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWGKLMfFGPPb",
        "outputId": "1b337d5e-b42b-4c2e-a56f-6f2fc2d15f02"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}